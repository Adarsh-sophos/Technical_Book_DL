\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\defcounter {refsection}{0}\relax 
\select@language {english}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {1}Preface}{5}{chapter.1}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {2}Acknowledgements}{7}{chapter.2}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {3}Introduction}{9}{chapter.3}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {4}Feedforward Neural Networks}{11}{chapter.4}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.1}Introduction}{12}{section.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.2}FNN architecture}{13}{section.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.3}Some notations}{14}{section.4.3}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.4}Weight averaging}{14}{section.4.4}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.5}Activation function}{15}{section.4.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.5.1}The sigmoid function}{15}{subsection.4.5.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.5.2}The tanh function}{16}{subsection.4.5.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.5.3}The ReLU function}{17}{subsection.4.5.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.5.4}The leaky-ReLU function}{18}{subsection.4.5.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.5.5}The ELU function}{19}{subsection.4.5.5}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.6}FNN layers}{20}{section.4.6}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.6.1}Input layer}{20}{subsection.4.6.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.6.2}Fully connected layer}{21}{subsection.4.6.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.6.3}Output layer}{21}{subsection.4.6.3}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.7}Loss function}{21}{section.4.7}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.8}Regularization techniques}{22}{section.4.8}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.8.1}L2 regularization}{22}{subsection.4.8.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.8.2}L1 regularization}{23}{subsection.4.8.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.8.3}Clipping}{24}{subsection.4.8.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.8.4}Dropout}{24}{subsection.4.8.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.8.5}Batch Normalization}{25}{subsection.4.8.5}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.9}Backpropagation}{27}{section.4.9}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.9.1}Backpropagate through Batch Normalization}{27}{subsection.4.9.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.9.2}error updates}{27}{subsection.4.9.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.9.3}Weight update}{28}{subsection.4.9.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.9.4}Coefficient update}{28}{subsection.4.9.4}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.10}Which data sample to use for gradient descent?}{29}{section.4.10}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.10.1}Full-batch}{29}{subsection.4.10.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.10.2}Stochastic Gradient Descent (SGD)}{29}{subsection.4.10.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.10.3}Mini-batch}{29}{subsection.4.10.3}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.11}Gradient optimization techniques}{30}{section.4.11}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.11.1}Momentum}{30}{subsection.4.11.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.11.2}Nesterov accelerated gradient}{30}{subsection.4.11.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.11.3}Adagrad}{31}{subsection.4.11.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.11.4}RMSprop}{31}{subsection.4.11.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.11.5}Adadelta}{31}{subsection.4.11.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.11.6}Adam}{32}{subsection.4.11.6}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.12}Weight initialization}{32}{section.4.12}
\defcounter {refsection}{0}\relax 
\contentsline {section}{Appendices}{33}{equation.4.12.73}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.A}Backprop through the output layer}{33}{section.1.4.A}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.B}Backprop through hidden layers}{34}{section.1.4.B}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.C}Backprop through BatchNorm}{34}{section.1.4.C}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.D}FNN ResNet (non standard presentation)}{35}{section.1.4.D}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.E}FNN ResNet (more standard presentation)}{38}{section.1.4.E}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.F}Matrix formulation}{38}{section.1.4.F}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {5}Convolutional Neural Networks}{41}{chapter.5}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.1}Introduction}{42}{section.5.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.2}CNN architecture}{43}{section.5.2}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.3}CNN specificities}{43}{section.5.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.3.1}Feature map}{43}{subsection.5.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.3.2}Input layer}{43}{subsection.5.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.3.3}Padding}{44}{subsection.5.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.3.4}Convolution}{45}{subsection.5.3.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.3.5}Pooling}{47}{subsection.5.3.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.3.6}Towards fully connected layers}{48}{subsection.5.3.6}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.3.7}fully connected layers}{48}{subsection.5.3.7}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.3.8}Output connected layer}{49}{subsection.5.3.8}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.4}Modification to Batch Normalization}{49}{section.5.4}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.5}Network architectures}{50}{section.5.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.5.1}Realistic architectures}{51}{subsection.5.5.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.5.2}LeNet}{52}{subsection.5.5.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.5.3}AlexNet}{52}{subsection.5.5.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.5.4}VGG}{53}{subsection.5.5.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.5.5}GoogleNet}{53}{subsection.5.5.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.5.6}ResNet}{54}{subsection.5.5.6}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.6}Backpropagation}{56}{section.5.6}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.6.1}Backpropagate through Batch Normalization}{57}{subsection.5.6.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.6.2}Error updates}{57}{subsection.5.6.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.6.2.1}Backpropagate from output to fc}{57}{subsubsection.5.6.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.6.2.2}Backpropagate from fc to fc}{58}{subsubsection.5.6.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.6.2.3}Backpropagate from fc to pool}{58}{subsubsection.5.6.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.6.2.4}Backpropagate from pool to conv}{59}{subsubsection.5.6.2.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.6.2.5}Backpropagate from conv to conv}{59}{subsubsection.5.6.2.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.6.2.6}Backpropagate from conv to pool}{60}{subsubsection.5.6.2.6}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.6.3}Weight update}{60}{subsection.5.6.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.6.3.1}Weight update from fc to fc}{60}{subsubsection.5.6.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.6.3.2}Weight update from fc to pool}{61}{subsubsection.5.6.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.6.3.3}Weight update from conv to conv}{61}{subsubsection.5.6.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.6.3.4}Weight update from conv to pool and conv to input}{61}{subsubsection.5.6.3.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.6.4}Coefficient update}{62}{subsection.5.6.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.6.4.1}Coefficient update from fc to fc}{62}{subsubsection.5.6.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.6.4.2}Coefficient update from fc to pool and conv to pool}{63}{subsubsection.5.6.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.6.4.3}Coefficient update from conv to conv}{63}{subsubsection.5.6.4.3}
\defcounter {refsection}{0}\relax 
\contentsline {section}{Appendices}{64}{equation.5.6.43}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.A}Backprop through BatchNorm}{64}{section.2.5.A}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.B}Error rate updates: details}{65}{section.2.5.B}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.C}Weight update: details}{67}{section.2.5.C}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.D}Coefficient update: details}{68}{section.2.5.D}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.E}Practical Simplification}{68}{section.2.5.E}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.E.1}pool to conv Simplification}{69}{subsection.2.5.E.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.E.2}Convolution Simplification}{70}{subsection.2.5.E.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.E.3}Coefficient Simplification}{71}{subsection.2.5.E.3}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.F}Batchpropagation through a ResNet module}{71}{section.2.5.F}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.G}Convolution as a matrix multiplication}{72}{section.2.5.G}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.G.1}2D Convolution}{73}{subsection.2.5.G.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.G.2}4D Convolution}{74}{subsection.2.5.G.2}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.H}Pooling as a row matrix maximum}{75}{section.2.5.H}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {6}Recurrent Neural Networks}{77}{chapter.6}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.1}Introduction}{78}{section.6.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.2}RNN-LSTM architecture}{78}{section.6.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.2.1}Forward pass in a RNN-LSTM}{78}{subsection.6.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.2.2}Backward pass in a RNN-LSTM}{80}{subsection.6.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.3}Extreme Layers and loss function}{80}{section.6.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.3.1}Input layer}{81}{subsection.6.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.3.2}Output layer }{81}{subsection.6.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.3.3}Loss function}{81}{subsection.6.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.4}RNN specificities}{81}{section.6.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.4.1}RNN structure}{81}{subsection.6.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.4.2}Forward pass in a RNN}{83}{subsection.6.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.4.3}Backpropagation in a RNN}{83}{subsection.6.4.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.4.4}Weight and coefficient updates in a RNN}{84}{subsection.6.4.4}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.5}LSTM specificities}{85}{section.6.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.5.1}LSTM structure}{85}{subsection.6.5.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.5.2}Forward pass in LSTM}{86}{subsection.6.5.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.5.3}Batch normalization}{87}{subsection.6.5.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.5.4}Backpropagation in a LSTM}{88}{subsection.6.5.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.5.5}Weight and coefficient updates in a LSTM}{89}{subsection.6.5.5}
\defcounter {refsection}{0}\relax 
\contentsline {section}{Appendices}{90}{equation.6.5.49}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.A}Backpropagation trough Batch Normalization}{90}{section.3.6.A}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.B}RNN Backpropagation}{91}{section.3.6.B}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.B.1}RNN Error rate updates: details}{91}{subsection.3.6.B.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.B.2}RNN Weight and coefficient updates: details}{93}{subsection.3.6.B.2}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.C}LSTM Backpropagation}{95}{section.3.6.C}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.C.1}LSTM Error rate updates: details}{95}{subsection.3.6.C.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.C.2}LSTM Weight and coefficient updates: details}{98}{subsection.3.6.C.2}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.D}Peephole connexions}{101}{section.3.6.D}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {7}Conclusion}{103}{chapter.7}
